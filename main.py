import os
from typing import Optional
from contextlib import asynccontextmanager

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, AIMessage

from my_agent import mage_graph



load_dotenv()



class ConversationMessage(BaseModel):
    """A single message in the conversation history."""
    role: str = Field(..., description="Message role: 'user' or 'assistant'")
    content: str = Field(..., description="Message content")


class ChatRequest(BaseModel):
    """Request body for the /chat endpoint."""
    user_id: str = Field(..., description="MongoDB user ID")
    user_name: str = Field(..., description="User's display name")
    conversation_history: Optional[list[ConversationMessage]] = Field(
        default=[],
        description="Previous messages in the conversation"
    )



class ChatResponse(BaseModel):
    """Response body for the /chat endpoint."""
    response: str = Field(..., description="The agent's response message")


class HealthResponse(BaseModel):
    """Response body for the /health endpoint."""
    status: str = Field(..., description="Health status")
    service: str = Field(..., description="Service name")
    version: str = Field(..., description="Service version")


# Sliding window size for conversation history
MAX_HISTORY_MESSAGES = 10


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler for startup/shutdown."""
    # Startup
    print("Summoning the mage...")
    
    yield
    
    # Shutdown
    print("The Mage is leaving...")


# Create FastAPI app
app = FastAPI(
    title="NotesMage Agent",
    description="the wizard assistant for NotesMage, powered by LangGraph",
    version="1.0.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[os.getenv("EXPRESS_SERVICE_URL", "http://localhost:5001")],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def convert_history_to_messages(
    history: list[ConversationMessage]
) -> list[HumanMessage | AIMessage]:
    """
    Convert conversation history from the request format to LangChain messages.
    Also applies sliding window to limit context size.
    """
    messages = []
    
    # Apply sliding window - keep only the last N messages
    truncated_history = history[-MAX_HISTORY_MESSAGES:] if history else []
    
    for msg in truncated_history:
        if msg.role == "user":
            messages.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            messages.append(AIMessage(content=msg.content))
    
    return messages



@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint for Render and monitoring.
    """
    return HealthResponse(
        status="healthy",
        service="the-mage",
        version="1.0.0"
    )


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint - processes user messages through The Mage agent.
    
    Flow:
    1. Convert conversation history to LangChain messages
    2. Run through the LangGraph (guard_rails → agent ↔ tools)
    3. Extract the final response and any actions taken
    4. Return the response
    """
    try:
        # Convert history to LangChain messages
        messages = convert_history_to_messages(request.conversation_history)
        
        # Prepare initial state
        initial_state = {
            "messages": messages,
            "user_id": request.user_id,
            "user_name": request.user_name
        }
        
        # Run the graph
        result = await mage_graph.ainvoke(initial_state)
        
        # Extract the final response
        final_messages = result.get("messages", [])
        if not final_messages:
            raise HTTPException(
                status_code=500,
                detail="No response generated by the agent"
            )
        
    
        last_message = final_messages[-1]
        
        # Handles both string content and list of content blocks (google vs groq vs deepseek responses!)
        if isinstance(last_message.content, str):
            response_content = last_message.content
        elif isinstance(last_message.content, list):
            # Extract text from content blocks
            text_parts = []
            for block in last_message.content:
                if isinstance(block, dict) and block.get('type') == 'text':
                    text_parts.append(block.get('text', ''))
                elif isinstance(block, str):
                    text_parts.append(block)
            response_content = '\n'.join(text_parts).strip()
        else:
            response_content = ""
        
        if not response_content:
            response_content = "I apologize, but I couldn't generate a response at the moment. Please try again later."
        
        
        return ChatResponse(response=response_content)
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in chat endpoint: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"An error occurred in the main chat endpoint: {str(e)}"
        )


# For local development
if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True,
    )
